{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "HYEt-XII37_cUhbfJvpxY",
      "metadata": {},
      "source": [
        "# Regularisation\n",
        "This exercise assumes that you have read the tutorial about regularisation and cross validation tutorial\n",
        ". You will use regularisation on the basis of the cross validation results to mitigate the effects overfitting.\n",
        "\n",
        "---\n",
        "**Task 1 (easy): Reflection on the tutorialüë©‚Äçüíª‚ôæÔ∏è**\n",
        "1. Run the cell in the tutorial implementing the hold-out train-validation split. \n",
        "2. Add a for-loop to rerun the code 20 times and store the $R^2$ results from each iteration. \n",
        "3. Calculate the mean and variance of the $R^2$ scores. Explain the results. \n",
        "4. Go back to the last part of tutorial and train the models with 3rd, 4th, and 5th order polynomials by using 10 fold cross validation. Does this affect the fit of the models? \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "R2 list:    [0.23582962646174288, 0.41494876246339296, 0.41280875857331867, -0.1756017273509758, 0.42962857245359853, 0.41270836833071356, 0.3268973960574004, 0.41402462153465336, 0.42390951358724593, 0.43548576587542087, 0.38885113320927134, 0.418277513507311, 0.4390928747939976, 0.39803776307184235, 0.46408196788770306, 0.4454053801601239, 0.3980978320986778, 0.363057455062746, 0.4134013175298582, 0.4445514487801008]\n",
        "mean:       0.3751747172044072\n",
        "variance:   0.36643324850736364\n",
        "```\n",
        "\n",
        "4.\n",
        "\n",
        "```\n",
        "R2 list:    [-31.4854684352986, -4678740.361226748, -3182677316.6941924]\n",
        "mean:       -1062452029.5136291\n",
        "variance:   6.743043847758225e+18\n",
        "```\n",
        "\n",
        "So what we see is a negative R2 value, which means that the model isn't capturing the underlying pattern at all, and its actually better to just take the average if the dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0axjYIHRz9TX14i3kvYi-",
      "metadata": {},
      "source": [
        "## Overview\n",
        "The following cell imports relevant libraries and sets up the dataset and model using the same configuration as in the tutorial:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "G6QHzjIvPl2YJkiTjcntP",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold, RepeatedKFold, cross_validate\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures, Normalizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import Ridge # additional import for regularization\n",
        "\n",
        "np.random.seed(99)\n",
        "\n",
        "dataset = fetch_california_housing(as_frame=True)\n",
        "\n",
        "df = dataset.frame # This is the dataframe (a table)\n",
        "\n",
        "X = dataset.data # These are the input features (anything but the house price)\n",
        "y = dataset.target # This contains the output features (just the house price)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LaZXHz0UHIGD9-AJajljM",
      "metadata": {},
      "source": [
        "## Regularization\n",
        "\n",
        "---\n",
        "**Task 2 (easy): Implementing regularizationüë©‚Äçüíª**\n",
        "In the tutorial, it was observed that incorporating the third or higher order polynomial features into a standard linear regression model leads to overfitting. In the following steps you will create a model pipeline similar to the one used in the tutorial using ridge regression.\n",
        "1. Create a third-order polynomial model with ridge regression (use the `Ridge`\n",
        " class imported from Scikit learn).\n",
        "2. Use the `np.geomspace`\n",
        " function to create an array, `regularization_params`\n",
        ", with values exponentially spaced between $10^{-10}$ and $10^2$. These values will be used to vary the regularization parameter. \n",
        "3. Train third-order Ridge regression models, by iterating over the elements in `regularization_params`\n",
        ". \n",
        "4. Store the test scores in an array, as they will be used in the next task.\n",
        "\n",
        "**Note:** Note: the regularization parameter $\\lambda$ in the lectures is called alpha in sckit learn.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "qk71mpxDQDb97NZUy4jrB",
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidParameterError",
          "evalue": "The 'steps' parameter of Pipeline must be an instance of 'list'. Got {('normalization', Normalizer()), ('features', PolynomialFeatures(degree=3)), ('model', Ridge(alpha=1e-10))} instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/IML_Introduction-to-Machine-Learning/iml-repo/vscode/02-material/W07/01-bias_variance.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672e636f64652d776f726b7370616365222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/IML_Introduction-to-Machine-Learning/iml-repo/vscode/02-material/W07/01-bias_variance.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m alpha \u001b[39min\u001b[39;00m regularization_params:\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672e636f64652d776f726b7370616365222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/IML_Introduction-to-Machine-Learning/iml-repo/vscode/02-material/W07/01-bias_variance.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m.\u001b[39mset_params(model__alpha\u001b[39m=\u001b[39malpha)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672e636f64652d776f726b7370616365222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/IML_Introduction-to-Machine-Learning/iml-repo/vscode/02-material/W07/01-bias_variance.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672e636f64652d776f726b7370616365222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/IML_Introduction-to-Machine-Learning/iml-repo/vscode/02-material/W07/01-bias_variance.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     test_scores\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mscore(X_test, y_test))\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672e636f64652d776f726b7370616365222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a22633a5c5c55736572735c5c4c756b617356696e746865724f6666656e6265725c5c7265706f735c5c6974755c5c696d6c5c5c494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e675c5c2e646576636f6e7461696e65725c5c646576636f6e7461696e65722e6a736f6e222c225f736570223a312c2265787465726e616c223a2266696c653a2f2f2f632533412f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f633a2f55736572732f4c756b617356696e746865724f6666656e6265722f7265706f732f6974752f696d6c2f494d4c5f496e74726f64756374696f6e2d746f2d4d616368696e652d4c6561726e696e672f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/IML_Introduction-to-Machine-Learning/iml-repo/vscode/02-material/W07/01-bias_variance.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(test_scores)\n",
            "File \u001b[0;32m/usr/local/conda/envs/iml/lib/python3.10/site-packages/sklearn/base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m partial_fit_and_fitted \u001b[39m=\u001b[39m (\n\u001b[1;32m   1141\u001b[0m     fit_method\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpartial_fit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1142\u001b[0m )\n\u001b[1;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m global_skip_validation \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1145\u001b[0m     estimator\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/usr/local/conda/envs/iml/lib/python3.10/site-packages/sklearn/base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    631\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    639\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[1;32m    640\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    641\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    642\u001b[0m     )\n",
            "File \u001b[0;32m/usr/local/conda/envs/iml/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'steps' parameter of Pipeline must be an instance of 'list'. Got {('normalization', Normalizer()), ('features', PolynomialFeatures(degree=3)), ('model', Ridge(alpha=1e-10))} instead."
          ]
        }
      ],
      "source": [
        "model = Pipeline({\n",
        "    (\"features\", PolynomialFeatures(3)),\n",
        "    (\"normalization\", Normalizer()),\n",
        "    (\"model\", Ridge()),\n",
        "})\n",
        "\n",
        "regularization_params = np.geomspace(10 ** -10, 10 ** 2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume X and y are your features and target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7)\n",
        "\n",
        "test_scores = []\n",
        "\n",
        "trained_models = []\n",
        "for alpha in regularization_params:\n",
        "    model.set_params(model__alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    test_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "print(test_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YqzuP6s9WWSOk2cU04-vg",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 3 (easy): Evaluating modelsüë©‚Äçüíª**\n",
        "1. Calculate the mean $R^2$ score for each model (each regularization value) and plot them.\n",
        "\n",
        "**Note:** Hint: It may be difficult to evaluate the small values. Use `plt.xscale('log')`\n",
        " to get evenly spaced points. \n",
        "\n",
        "2. Calculate the standard deviation of the scores for each model and plot them. \n",
        "\n",
        "**Note:** Hint: use logarithmic scales for both the x- and y-axis.\n",
        "\n",
        "3. Based on the generated plots, which regularization parameter value gives the best results and why? Note down your observations and reflections in the text field below as it will be used in the next task.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MdE-l5yzT9S6mVU0clheq",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qf6X3dM8a79LPFfHOfObQ",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 4 (medium): Cross validationüë©‚Äçüíª**\n",
        "This task investigates model generalization using k-fold cross validation.\n",
        "1. Construct a new model, with the same setup as before by using the optimal regularization parameter found in the previous task. \n",
        "2. Train the model using k-fold cross validation. Set the number of folds to 2.\n",
        "3. Vary the number of folds from 2 to 20 and store the mean and the standard deviation of the $R^2$ score for each fold. \n",
        "4. Plot the mean and the standard deviation of the $R^2$  scores.\n",
        "5. (Optional) This task uses the `RepeatedKFold`\n",
        " function to obtain a more robust evaluation of model performance. `RepeatedKFold`\n",
        " repeats k-fold cross-validation 10 times by default. The folds are chosen randomly for each repetition. The runtime can be reduced by decreasing the number of repetitions (`n_repeats`\n",
        " parameter).\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BACz6w2vDmm2bXkN30C-E",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rUjarYVaVqvzCMZ6qTcDg",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 5 (medium): Reflection on resultsüí°**\n",
        "1. Use the plotted mean and variance to argue for model performance. \n",
        "2. List reasons for the variability in model performance? \n",
        "3. Compare the variability in model perfomance observed in the tutorial with the results of the current exercise.\n",
        "4. Argue how the regularized model performs compared to the standard linear regression implemented in the tutorial.     - Print the model parameters and use them to argue for differences between the linear model and the regularized model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
